{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc896af-471a-4c2f-adb9-70bdcb9bed8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268bf32a-0361-4227-bdb3-da87bc6df946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuração de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# configuração\n",
    "spark = SparkSession.builder.appName(\"BikeSampa-API\").getOrCreate()\n",
    "\n",
    "# pegando a data para colocar no nome do arquivo\n",
    "data_atual = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# url api\n",
    "url = 'https://api.citybik.es/v2/networks/bikesampa'\n",
    "\n",
    "# caminhos para salvar\n",
    "caminho_bronze = \"/Volumes/projeto/default/bronze\"\n",
    "caminho_erro = \"/Volumes/projeto/default/erros/bikesampaerro.parquet\"\n",
    "caminho_bronze_bike_data = f\"{caminho_bronze}/bike_{data_atual}.parquet\"\n",
    "\n",
    "# estruturando o dataset\n",
    "bike_schema = StructType([\n",
    "    StructField(\"empty_slots\", IntegerType(), True),\n",
    "    StructField(\"free_bikes\", IntegerType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "def process_bikes_data(stations):\n",
    "    # garantir que dados sao validos\n",
    "    if not all(isinstance(s, dict) and 'id' in s and 'name' in s for s in stations):\n",
    "        raise ValueError(\"Dados da estação estão no formato incorreto ou incompleto.\")\n",
    "    \n",
    "    # cria o df com dados verificados\n",
    "    return spark.createDataFrame(stations, schema=bike_schema)\n",
    "\n",
    "try:\n",
    "    # requisitando o bikesampa\n",
    "    logger.info(\"Iniciando requisição para a API BikeSampa...\")\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Verifica se a resposta tem dados\n",
    "    stations = response.json().get('network', {}).get('stations', [])\n",
    "    if not stations:\n",
    "        raise ValueError(\"Nenhum dado de estação foi encontrado na resposta da API.\")\n",
    "    \n",
    "    logger.info(f\"{len(stations)} estações encontradas.\")\n",
    "\n",
    "    # Processando os dados da estação\n",
    "    df_bikes = process_bikes_data(stations)\n",
    "\n",
    "    # tenta salvar no bronze e na tabela\n",
    "    try:\n",
    "        logger.info(f\"Salvando dados no formato Parquet em {caminho_bronze}...\")\n",
    "        df_bikes.write.mode(\"overwrite\").parquet(caminho_bronze_bike_data)\n",
    "\n",
    "        logger.info(\"Salvando dados na tabela 'bike_tabela'...\")\n",
    "        df_bikes.write.mode(\"overwrite\").saveAsTable(\"bike_tabela\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao salvar os dados no formato Parquet ou na tabela: {e}\")\n",
    "        raise\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    logger.error(f\"Erro ao fazer requisição para a API: {e}\")\n",
    "    # cria DataFrame de erro\n",
    "    df_bikes = spark.createDataFrame([], bike_schema).withColumn(\"erro_carregamento\", lit(str(e)))\n",
    "    df_bikes.write.mode(\"overwrite\").parquet(caminho_erro)\n",
    "\n",
    "except ValueError as e:\n",
    "    logger.error(f\"Erro de valor: {e}\")\n",
    "    # cria DataFrame de erro\n",
    "    df_bikes = spark.createDataFrame([], bike_schema).withColumn(\"erro_carregamento\", lit(str(e)))\n",
    "    df_bikes.write.mode(\"overwrite\").parquet(caminho_erro)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro desconhecido: {e}\")\n",
    "    # cria DataFrame de erro\n",
    "    df_bikes = spark.createDataFrame([], bike_schema).withColumn(\"erro_carregamento\", lit(str(e)))\n",
    "    df_bikes.write.mode(\"overwrite\").parquet(caminho_erro)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bike camada bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
